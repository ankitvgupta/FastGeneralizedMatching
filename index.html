<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>FastGeneralizedMatching by ankitvgupta</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" href="main.css" type="text/css" media="screen" />
  </head>
  <body>

    <section class="page-header">
      <h1 class="project-name">FastGeneralizedMatching</h1>
      <h2 class="project-tagline">New algorithms and implementations of parallelized matching in Spark.</h2>
      <a href="https://github.com/ankitvgupta/FastGeneralizedMatching" class="btn">View on GitHub</a>
      <a href="https://github.com/ankitvgupta/FastGeneralizedMatching/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/ankitvgupta/FastGeneralizedMatching/tarball/master" class="btn">Download .tar.gz</a>
    </section>

          <ul id="toc">
<li><span>What is the problem we are trying to solve?</span> <a href="#problem">Problem</a></li>
<li><span>Our data in detail</span> <a href="#data-overview">Data Overview</a></li>
<li><span>Our program design and feature choice</span> <a href="#program-design">Program Design</a></li>
<li><span>How to use our application</span> <a href="#usage">Usage</a></li>
<li><span>How performant is our code?</span> <a href="#performance">Performance</a></li>
<li><span>What interesting insights did we gain from this project?</span> <a href="#insights">Insights</a></li>
<li><span>What extensions and improvments might we work on?</span> <a href="#extensions">Extensions</a></li>
<li><span>What was most enjoyable, challenging, and frustrating? </span> <a href="#takeaways">Takeaways</a></li>
</ul>

    <section class="main-content">



      <h3>
<a id="problem" class="anchor" href="#problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problem</h3>

<p>For our final project, we explored several problems in market design, a field of econonmics which concerns optimally matching two or more sets of agents who have preferences about the other. In particular, we are exploring the subset of market design problems known as “matching”. One of the central problems in matching is the “stable marriage” problem, in which there are N men and N women, each of which have an order of preferences over the other. The goal is to find N pairings such that there is no unpaired couple that both want each other more than their current partners. </p>

<p>There is a known serial algorithm to solve this known as the Gale-Shapley Algorithm.  In this simple algorithm, the men who do not have a match yet propose to the women, and the women non-bindingly 'accept' the man who proposed to them that they prefer the most.  This continues until every man has either proposed to their list of women or is matched to a woman. Its correctness (in terms of producing stable matchings, a term we will define below) can be argued succinctly by contradiction, and its termination in at most $N^2$ iterations can be argued by noting that in the worst case every man proposes to every woman, a total of $N^2$ events.  Despite its simplicity, this algorithm has found hundreds of areas of application in theoretical and applied economics: including various public school system matchings, matchings of kidney donors with those in need, and many, many more. For these achievements, this algorithm garnered Gale and Shapley the nobel prize.</p>

<p>An extension to this problem is used in the National Residency Match, a real process by which doctors finishing medical school get placed into American residency programs. In this set, each doctor has an ordering of preferences over some subset of the hospitals. Each hospital has an ordering over the doctors that apply there, and some number of spots. The goal is to again find a matching such that no unpaired (doctor, hospital) both want each other more than their current match. It is thought in the literature that this problem is solved using an adaptation of the serial Gale-Shapley algorithm. </p>

<p>Furthermore, there is a field of market design called matching with contracts, where one problem setup is that rather than having preferences over each other, each doctor has a price they are willing to be paid to work at each hospital, and each hospital has a utility for each doctor, and we are trying to find the best assignment of doctors to hospitals such that each hospital maximizes its utility while keeping within its budget.  </p>

<p>We believe that the scale of these problems means that developing a scalable parallel algorithm will be useful. In particular, our project is focused on developing parallel Spark-based algorithms for solving the National Residency Match and related matching problems, such as the general problem of college admissions, and, as a reach goal, to work towards solutions for the matching with contracts problem, which has exponential complexity and no known sub-exponential solutions. </p>

<h3>
<a id="data-overview" class="anchor" href="#data-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data Overview</h3>

<h4>
<a id="describe-your-data-in-detail-where-did-it-come-from-how-did-you-acquire-it-what-does-it-mean-etc" class="anchor" href="#describe-your-data-in-detail-where-did-it-come-from-how-did-you-acquire-it-what-does-it-mean-etc" aria-hidden="true"><span class="octicon octicon-link"></span></a>Describe your data in detail: where did it come from, how did you acquire it, what does it mean, etc.</h4>

<p> We discussed the problem with a Professor who does research in market design, which lead to our deciding to attempt to generate sample data for the National Residency Match.  The data for our problem was modeled off of the parameters of the National Residency Match. We know from the match's publications that there are approximately 41,000 doctors that are matched to 5,000 hospitals to up to 30,000 spots. Thus, we generated data using these parameters. In particular, we generate a random set of preferences for each doctor, and for each hospital. </p>

<p>Thus, the input data representation essentially consistent of two files. There are N doctors and M hospitals, and each doctor can apply to up to numHospitalsPerDoctor hospitals.</p>

<p>1 doctor_preferences: N lines. Each line has an random permutation of numHospitalsPerDoctor hospital ids, each of which is between 0 and M - 1.</p>

<p>2 hospital_preferences: M lines. Each line has a random permutation of the doctors that applied there.</p>

<h3>
<a id="program-design" class="anchor" href="#program-design" aria-hidden="true"><span class="octicon octicon-link"></span></a>Program Design</h3>

<p>There were many phases of our project. The first phase was to design an implementation for the Gale-Shapley algorithm that was as fast as possible, and to design a fast method to verify stability. While the algorithm is easy to find, including on Wikipedia, we wanted to do our best to make it as fast as possible, so that we were making a fair comparison between the parallel and serial implementations. </p>

<p>To accomplish this, we first had to design a technique to generate preferences for each doctor over hospitals, and for each hospital over the doctors that apply. Thus, we wrote a program that generates these preferences, given some set of size parameters, that we used to change the problem size and compare the implementation speeds. Essentially, given some parameters, this script would generate doctor_preferences.txt and hospital_preferences.txt, which are respectively, the preferences of each doctor and each hospital over each other.</p>

<p>Once we were satisfied that we had written a fast serial solver, we decided to work towards a parallel implementation. We chose to use Spark because it allows us to quickly scale this problem well, especially given that we were working with files that were enormous in size, and thus not best-suited for the other parallel techniques we explored. </p>

<p>The first Spark-based algorithm we wrote modeled Gale-Shapley, but parallelized the assignment of doctors to hospitals. As a reminder, the high level way that Gale-Shapley works is that each man proposes to the top-ranked woman that he has not yet proposed to, and if that man is higher ranked than the woman's current partner, he engages her. This process repeats until all of the men have tried each of their preferences, or are fully matched. Our parallel algorithm accomplishes this by proposing to all of the women in parallel. Then, each woman selects the top man among those that propose to her. The select men are now engaged. The rest repeat the process, but look at their next higher preference. This process ends once all of the unmatched men have tried all of their preferences. To apply this to the National Residency Match, the "man" is the "doctor" and the "woman" is the "hospital", and rather than each woman only being able to "engage" one man, each "hospital" can "accept" multiple "doctors". So, rather than taking the top-ranked doctor, each hospital will take the top K ranked doctors, where K is the number of spots in a hospital.</p>

<p>We found that this technique did not scale very well. On a Macbook Pro, we could not generate a dataset in which this beats the serial implementation. So, we decided to completely rethink how we approach this parallelization. After thinking hard, we realized that the reason this technique was slow was that it still required iterating over all of the preferences for the unmatched doctors. Thus, if there are 300 preferences, it would still require 300 iterations. So, we set out to design an algorithm that parallelized over not just the assignment of doctors to hospital for a set of preferences, but to parallelize over all of those preferences themselves.</p>

<p>This is exactly what we did. Our new algorithm is as follows, and is inspired by how we imagine this matching occurs in a college admissions process (albeit much faster!). Each doctor applies to all of her preferences at once. Then, each hospital picks the top N doctors among those that applied there, and send "acceptance letters" to the doctors. Then, each doctor picks his top-ranked hospital among the places he got accepted, and informs the hospitals of this decision. At this point, you can imagine that each doctor has gotten "tentatively accepted" to some hospital, or denied from all of this choices, and therefore "waitlisted". However, since some doctors may have rejected offers, a space may have opened up. So, we truncate the list of preferences for each hospital to only include the hospitals up until the one they were accepted to. So, if a doctor has the preferences Hosp1 &gt; Hosp2 &gt; Hosp3 &gt; Hosp4, and get's into Hosp3, their new preferences would be Hosp1 &gt; Hosp2 &gt; Hosp3. Then, we repeat the process until there are no changes in the matchings. The intuition behind this is that each doctor will repeatedly try to get into a better hospital than the one they are currently in.</p>

<p>Critically, in this case we found that for N doctors, it took around logN iterations to reach stable matchings, and thus we could beat the serial implementation on small and large datasets.</p>

<p>Describe your program design and why you chose the features you did.</p>

<h3>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span class="octicon octicon-link"></span></a>Usage</h3>

<p>To use this application, run the main program takes in a list of preferences for each doctor and hospital. So, if this were to be used for the true National Residency Match, essentially the user would just have to move all of the data into the appropriate files, and then call the Python-based Spark program. Then, the program generates the matchings, and verifies that they are all stable. Lastly, it can output the resultant matchings, if required.  The only dependency is PySpark. </p>

<h3>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance</h3>

<p>What is the performance of your code? What speedup and efficiency did you achieve? What optimizations did you implement to achieve this speedup?</p>

<h3>
<a id="insights" class="anchor" href="#insights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Insights</h3>

<p>We found a number of interesting insights. For one, it suggests that for relatively small sized problems, Spark has enormous overhead in achieving its parallel behavior, which makes it not as useful. In particular, when operating Spark on a Macbook Pro with 4 cores, we found limited speedup in conducting the National Residency Match using Spark. However, once the size of the dataset got larger, the Spark-based algorithm very quickly caught up and overtook the serial implementation.</p>

<p>Furthermore, our intuitions about where the complexity of this problem was evolved over time. Earlier, our main expectation of what made this problem hard was the fact that there are thousands of doctors trying to be matched to thousands of hospitals, and thus being able to parallelize this process would quickly lead to a significant speedup. While this was true to some extent, we actually found that there was a huge deal of complexity caused by the potentially high number of hospitals that each doctor to apply to. When we increased the number of hospitals each doctor could apply to, we saw polynomial slowdown in the amount of time that the serial implementation took. Furthermore, when we wrote our second Spark-based algorithm which allowed each doctor to submit all of their choices at once, we got significant speedups, which makes sense since it mitigated for a single doctor making many applications.</p>

<img src="alg1_vs_alg2.png"/>

<p>The comparison between the parallel algorithm and the serial algorithm is striking.  Really there is hardly a comparison to be made between the two -- at any nontrivial problem size
the overhead of Spark and the extra overhead of our algorithm is overcome by the enormous scalability of our algorithm.  Note that all of our data was obtained by running our code on AWS, on a [ANKIT INSERT HERE] cluster. </p>

<img src="parallel_vs_serial.png"/>

<h3>
<a id="extensions" class="anchor" href="#extensions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extensions</h3>

<p>One of the extensions that we are currently working on is to building in support for a much more complicated class of problems called Matching with Contracts. In this class of problems, rather than the doctors and the hospitals each having a set of preferences over one another, one version is that each doctor has a price that they are willing to work at each hospital, and each hospital has a budget. Hospitals are trying to maximize their utility while staying within budget. However, this problem, even as a relatively simply form of the more general matching with contracts more, still has exponential complexity, and is extremely difficult to produce provable stable matchings for. Furthermore, few people have worked in this space before. Thus, it would be interesting to explore whether we can get within provable bounded closeness to the true optimums in this case.
What extensions and improvements can you suggest?</p>

<h3>
<a id="takeaways" class="anchor" href="#takeaways" aria-hidden="true"><span class="octicon octicon-link"></span></a>Takeaways</h3>

<p>We definitely found, as we began to see in some of the earlier problem sets, that working in a cluster-computing environment requires deeply understanding the parts of the problem where the serial implementation is leading to the most computation complexity that can be parallelized, and focusing on rethinking that approach. Moreover, we found that even when some parts of serial algorithms can be clearly parallelized, this technique does not necessarily lead to great parallel algorithms, and instead fundamental aspects of the algorithmic design need to be rethought and challenged. Thinking about how to rethink algorithms from the ground up - many of which have been highly studied for decades - so that they can be very efficiency implemented in a parallel environment was definitely the most challenging part.</p>

<p>The most frustrating part was thinking through the nuances of Spark to make sure that any potential slowdown in operation was due to fundamental complexity, and not due to suboptimal caching or premature materialization. </p>

<p>Next time, rather than first thinking about serial implementations and trying to adapt them in a parallel way, we would effectively try to start from the ground up on these approaches. That would allow us to think about the problem space with a Parallel-first mindset, and likely get us to the true solution even faster.</p>

<p>What did you most enjoy about working on this project? What was the most challenging aspect? What was the most frustrating? What would you do differently next time?</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ankitvgupta/FastGeneralizedMatching">FastGeneralizedMatching</a> is maintained by <a href="https://github.com/ankitvgupta">ankitvgupta</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
