{"name":"FastGeneralizedMatching","tagline":"A repository for an implementation of Matching with Contracts. Initial work will be applied to CS 205 Final Project at Harvard University.","body":"### Problem\r\nFor our final project, we are exploring several market design problems. In particular, we are exploring the subset of market design problems known as “matching”. One of the central problems in matching is the “stable marriage” problem, in which there are N men and N women, each of which have an order of preferences over the other. The goal is to find N pairings such that there is no unpaired couple that both want each other more than their current partners. There is a known serial algorithm to solve this known as the Gale-Shapley Algorithm.\r\n\r\nAn extension to this problem is used in the National Residency Match, a real process by which doctors finishing medical school get placed into American residency programs. In this set, each doctor has an ordering of preferences over some subset of the hospitals. Each hospital has an ordering over the doctors that apply there, and some number of spots. The goal is to again find a matching such that no unpaired (doctor, hospital) both want each other more than their current match. It is thought in the literature that this problem is solved using an adaptation of the serial Gale-Shapley algorithm. \r\n\r\nFurthermore, there is a field of market design called matching with contracts, where one problem setup is that rather than having preferences over each other, each doctor has a price they are willing to be paid to work at each hospital, and each hospital has a utility for each doctor, and we are trying to find the best assignment of doctors to hospitals such that each hospital maximizes its utility while keeping within its budget.  \r\n\r\nWe believe that the scale of these problems means that developing a parallel algorithm can solve this class of problems well. In particular, our project is focused on developing parallel Spark-based algorithms for solving the National Residency Match, and, as a reach goal, to work towards solutions for the matching with contracts problem, which has exponential complexity and no known non-trivial serial algorithmic solutions.\r\n\r\n\r\n### Data Overview\r\n####Describe your data in detail: where did it come from, how did you acquire it, what does it mean, etc.\r\n\r\nThe data for our problem was modeled off of the parameters of the National Residency Match. We know from the match's publications that there are approximately 41,000 doctors that are matched to 5,000 hospitals to up to 30,000 spots. Thus, we generated data using these parameters. In particular, we generate a random set of preferences for each doctor, and for each hospital. \r\n\r\nThus, the input data representation essentially consistent of two files. There are N doctors and M hospitals, and each doctor can apply to up to numHospitalsPerDoctor hospitals.\r\n\r\n1 doctor_preferences: N lines. Each line has an random permutation of numHospitalsPerDoctor hospital ids, each of which is between 0 and M - 1.\r\n\r\n2 hospital_preferences: M lines. Each line has a random permutation of the doctors that applied there.\r\n\r\n### Program Design\r\n\r\n\r\nDescribe your program design and why you chose the features you did.\r\n\r\n\r\n### Usage\r\nTo use this application, the main program takes in a list of preferences for each doctor and hospital. So, if this were to be used for the true National Residency Match, essentially the user would just have to move all of the data into the appropriate files, and then call the Python-based Spark program. Then, the program generates the matchings, and verifies that they are all stable. Lastly, it can output the resultant matchings, if required. \r\nHow do you use your application (mouse and keyboard functions, input/output, etc)?\r\n\r\n\r\n### Performance\r\n\r\nWhat is the performance of your code? What speedup and efficiency did you achieve? What optimizations did you implement to achieve this speedup?\r\n### Insights\r\nWe found a number of interesting insights. For one, it suggests that for relatively small sized problems, Spark has enormous overhead in achieving its parallel behavior, which makes it not as useful. In particular, when operating Spark on a Macbook Pro with 4 cores, we found limited speedup in conducting the National Residency Match using Spark. However, once the size of the dataset got larger, the Spark-based algorithm very quickly caught up and overtook the serial implementation.\r\n\r\nFurthermore, our intuitions about where the complexity of this problem was evolved over time. Earlier, our main expectation of what made this problem hard was the fact that there are thousands of doctors trying to be matched to thousands of hospitals, and thus being able to parallelize this process would quickly lead to a significant speedup. While this was true to some extent, we actually found that there was a huge deal of complexity caused by the potentially high number of hospitals that each doctor to apply to. When we increased the number of hospitals each doctor could apply to, we saw polynomial slowdown in the amount of time that the serial implementation took. Furthermore, when we wrote our second Spark-based algorithm which allowed each doctor to submit all of their choices at once, we got significant speedups, which makes sense since it mitigated for a single doctor making many applications.\r\n\r\nWhat interesting insights did you gain from this project?\r\n\r\n\r\n### Extensions\r\nOne of the extensions that we are currently working on is to building in support for a much more complicated class of problems called Matching with Contracts. In this class of problems, \r\nWhat extensions and improvements can you suggest?\r\n### Takeaways\r\nWhat did you most enjoy about working on this project? What was the most challenging aspect? What was the most frustrating? What would you do differently next time?\r\n\r\n\r\n### Welcome to GitHub Pages.\r\nTesting\r\nThis automatic page generator is the easiest way to create beautiful pages for all of your projects. Author your page content here [using GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/), select a template crafted by a designer, and publish. After your page is generated, you can check out the new `gh-pages` branch locally. If you’re using GitHub Desktop, simply sync your repository and you’ll see the new branch.\r\n\r\n### Designer Templates\r\nWe’ve crafted some handsome templates for you to use. Go ahead and click 'Continue to layouts' to browse through them. You can easily go back to edit your page before publishing. After publishing your page, you can revisit the page generator and switch to another theme. Your Page content will be preserved.\r\n\r\n### Creating pages manually\r\nIf you prefer to not use the automatic generator, push a branch named `gh-pages` to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}