{"name":"FastGeneralizedMatching","tagline":"A repository for an implementation of Matching with Contracts. Initial work will be applied to CS 205 Final Project at Harvard University.","body":"### Problem\r\nFor our final project, we are exploring several market design problems. In particular, we are exploring the subset of market design problems known as “matching”. One of the central problems in matching is the “stable marriage” problem, in which there are N men and N women, each of which have an order of preferences over the other. The goal is to find N pairings such that there is no unpaired couple that both want each other more than their current partners. There is a known serial algorithm to solve this known as the Gale-Shapley Algorithm.\r\n\r\nAn extension to this problem is used in the National Residency Match, a real process by which doctors finishing medical school get placed into American residency programs. In this set, each doctor has an ordering of preferences over some subset of the hospitals. Each hospital has an ordering over the doctors that apply there, and some number of spots. The goal is to again find a matching such that no unpaired (doctor, hospital) both want each other more than their current match. It is thought in the literature that this problem is solved using an adaptation of the serial Gale-Shapley algorithm. \r\n\r\nFurthermore, there is a field of market design called matching with contracts, where one problem setup is that rather than having preferences over each other, each doctor has a price they are willing to be paid to work at each hospital, and each hospital has a utility for each doctor, and we are trying to find the best assignment of doctors to hospitals such that each hospital maximizes its utility while keeping within its budget.  \r\n\r\nWe believe that the scale of these problems means that developing a parallel algorithm can solve this class of problems well. In particular, our project is focused on developing parallel Spark-based algorithms for solving the National Residency Match, and, as a reach goal, to work towards solutions for the matching with contracts problem, which has exponential complexity and no known non-trivial serial algorithmic solutions.\r\n\r\n\r\n### Data Overview\r\n####Describe your data in detail: where did it come from, how did you acquire it, what does it mean, etc.\r\n\r\nThe data for our problem was modeled off of the parameters of the National Residency Match. We know from the match's publications that there are approximately 41,000 doctors that are matched to 5,000 hospitals to up to 30,000 spots. Thus, we generated data using these parameters. In particular, we generate a random set of preferences for each doctor, and for each hospital. \r\n\r\nThus, the input data representation essentially consistent of two files. There are N doctors and M hospitals, and each doctor can apply to up to numHospitalsPerDoctor hospitals.\r\n\r\n1 doctor_preferences: N lines. Each line has an random permutation of numHospitalsPerDoctor hospital ids, each of which is between 0 and M - 1.\r\n\r\n2 hospital_preferences: M lines. Each line has a random permutation of the doctors that applied there.\r\n\r\n### Program Design\r\n\r\nThere were many phases of our project. The first phase was to design an implementation for the Gale-Shapley algorithm that was as fast as possible, and to design a method to verify stability fast. While the algorithm is easy to find, including on Wikipedia, we wanted to do our best to make it as fast as possible, so that we were making a fair comparison between the parallel and serial implementations. \r\n\r\nTo accomplish this, we first had to design a technique to generate preferences for each doctor over hospitals, and for each hospital over the doctors that apply. Thus, we wrote a program that generates these preferences, given some set of size parameters, that we used to change the problem size and compare the implementation speeds. Essentially, given some parameters, this script would generate doctor_preferences.txt and hospital_preferences.txt, which are respectively, the preferences of each doctor and each hospital over each other.\r\n\r\nOnce we were satisfied that we had written a fast serial solver, we decided to work towards a parallel implementation. We chose to use Spark because it allows us to quickly scale this problem well, especially given that we were working with files that were enormous in size, and thus not best-suited for the other parallel techniques we explored. \r\n\r\nThe first Spark-based algorithm we wrote modeled Gale-Shapley, but parallelized the assignment of doctors to hospitals. Essentially, the high level way that Gale-Shapley works is that each man proposes to the top-ranked woman that he has not yet proposed to, and if that man is higher ranked than the woman's current partner, he engages her. This process repeats until all of the men have tried each of their preferences, or are fully matched. Our parallel algorithm accomplishes this by proposing to all of the women in parallel. Then, each woman selects the top man among those that propose to her. The select men are now engaged. The rest repeat the process, but look at their next higher preference. This process ends once all of the unmatched men have tried all of their preferences. To apply this to the National Residency Match, the \"man\" is the \"doctor\" and the \"woman\" is the \"hospital\", and rather than each woman only being able to \"engage\" one man, each \"hospital\" can \"accept\" multiple \"doctors\". So, rather than taking the top-ranked doctor, each hospital will take the top K ranked doctors, where K is the number of spots in a hospital.\r\n\r\nWe found that this technique did not scale very well. On a Macbook Pro, we could not generate a dataset in which this beats the serial implementation. So, we decided to completely rethink how we approach this parallelization. After thinking hard, we realized that the reason this technique was slow was that it still required iterating over all of the preferences for the unmatched doctors. Thus, if there are 300 preferences, it would still require 300 iterations. So, we set out to design an algorithm that parallelized over not just the assignment of doctors to hospital for a set of preferences, but to parallelize over all of those preferences themselves.\r\n\r\nThis is exactly what we did. Our new algorithm is as follows, and is inspired by how we imagine this matching occurs in a college admissions process (albeit much faster!). Each doctor applies to all of her preferences at once. Then, each hospital picks the top N doctors among those that applied there, and send \"acceptance letters\" to the doctors. Then, each doctor picks his top-ranked hospital among the places he got accepted, and informs the hospitals of this decision. At this point, you can imagine that each doctor has gotten \"tentatively accepted\" to some hospital, or denied from all of this choices, and therefore \"waitlisted\". However, since some doctors may have rejected offers, a space may have opened up. So, we truncate the list of preferences for each hospital to only include the hospitals up until the one they were accepted to. So, if a doctor has the preferences Hosp1 > Hosp2 > Hosp3 > Hosp4, and get's into Hosp3, their new preferences would be Hosp1 > Hosp2 > Hosp3. Then, we repeat the process until there are no changes in the matchings. The intuition behind this is that each doctor will repeatedly try to get into a better hospital than the one they are currently in.\r\n\r\nCritically, in this case we found that for N doctors, it took around logN iterations to reach stable matchings, and thus we could beat the serial implementation on small and large datasets.\r\n\r\n\r\n\r\nDescribe your program design and why you chose the features you did.\r\n\r\n\r\n### Usage\r\nTo use this application, the main program takes in a list of preferences for each doctor and hospital. So, if this were to be used for the true National Residency Match, essentially the user would just have to move all of the data into the appropriate files, and then call the Python-based Spark program. Then, the program generates the matchings, and verifies that they are all stable. Lastly, it can output the resultant matchings, if required. \r\nHow do you use your application (mouse and keyboard functions, input/output, etc)?\r\n\r\n\r\n### Performance\r\n\r\nWhat is the performance of your code? What speedup and efficiency did you achieve? What optimizations did you implement to achieve this speedup?\r\n### Insights\r\nWe found a number of interesting insights. For one, it suggests that for relatively small sized problems, Spark has enormous overhead in achieving its parallel behavior, which makes it not as useful. In particular, when operating Spark on a Macbook Pro with 4 cores, we found limited speedup in conducting the National Residency Match using Spark. However, once the size of the dataset got larger, the Spark-based algorithm very quickly caught up and overtook the serial implementation.\r\n\r\nFurthermore, our intuitions about where the complexity of this problem was evolved over time. Earlier, our main expectation of what made this problem hard was the fact that there are thousands of doctors trying to be matched to thousands of hospitals, and thus being able to parallelize this process would quickly lead to a significant speedup. While this was true to some extent, we actually found that there was a huge deal of complexity caused by the potentially high number of hospitals that each doctor to apply to. When we increased the number of hospitals each doctor could apply to, we saw polynomial slowdown in the amount of time that the serial implementation took. Furthermore, when we wrote our second Spark-based algorithm which allowed each doctor to submit all of their choices at once, we got significant speedups, which makes sense since it mitigated for a single doctor making many applications.\r\n\r\nWhat interesting insights did you gain from this project?\r\n\r\n\r\n### Extensions\r\nOne of the extensions that we are currently working on is to building in support for a much more complicated class of problems called Matching with Contracts. In this class of problems, rather than the doctors and the hospitals each having a set of preferences over one another, one version is that each doctor has a price that they are willing to work at each hospital, and each hospital has a budget. Hospitals are trying to maximize their utility while staying within budget. However, this problem, even as a relatively simply form of the more general matching with contracts more, still has exponential complexity, and is extremely difficult to produce provable stable matchings for. Furthermore, few people have worked in this space before. Thus, it would be interesting to explore whether we can get within provable bounded closeness to the true optimums in this case.\r\nWhat extensions and improvements can you suggest?\r\n\r\n### Takeaways\r\nWe definitely found, as we began to see in some of the earlier problem sets, that working in a cluster-computing environment requires deeply understanding the parts of the problem where the serial implementation is leading to the most computation complexity that can be parallelized, and focusing on rethinking that approach. Moreover, we found that even when some parts of serial algorithms can be clearly parallelized, this technique does not necessarily lead to great parallel algorithms, and instead fundamental aspects of the algorithmic design need to be rethought and challenged. Thinking about how to rethink algorithms from the ground up - many of which have been highly studied for decades - so that they can be very efficiency implemented in a parallel environment was definitely the most challenging part.\r\n\r\nThe most frustrating part was thinking through the nuances of Spark to make sure that any potential slowdown in operation was due to fundamental complexity, and not due to suboptimal caching or premature materialization. \r\n\r\nNext time, rather than first thinking about serial implementations and trying to adapt them in a parallel way, we would effectively try to start from the ground up on these approaches. That would allow us to think about the problem space with a Parallel-first mindset, and likely get us to the true solution even faster.\r\n\r\nWhat did you most enjoy about working on this project? What was the most challenging aspect? What was the most frustrating? What would you do differently next time?\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}